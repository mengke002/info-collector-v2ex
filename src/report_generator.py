"""
V2EXæ™ºèƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
åŸºäºèŠ‚ç‚¹çš„çƒ­ç‚¹å†…å®¹åˆ†æå’ŒMarkdownæŠ¥å‘Šç”Ÿæˆ
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta

from .database import db_manager
from .llm_client import llm_client


class V2EXReportGenerator:
    """V2EXæ™ºèƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.db = db_manager
        self.llm = llm_client
        
        # æŠ¥å‘Šé…ç½®
        self.top_topics_per_node = 30
        self.top_replies_per_topic = 10
        self.max_content_length = 50000
    
    def get_beijing_time(self) -> datetime:
        """è·å–å½“å‰åŒ—äº¬æ—¶é—´"""
        return datetime.now(timezone.utc) + timedelta(hours=8)
    
    def _truncate_content_for_logging(self, content: str, max_length: int = 300) -> str:
        """
        ä¸ºæ—¥å¿—è®°å½•æˆªå–å†…å®¹ï¼Œé¿å…æš´éœ²è¿‡å¤šæ•æ„Ÿä¿¡æ¯
        
        Args:
            content: è¦æˆªå–çš„å†…å®¹
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æˆªå–åçš„å†…å®¹
        """
        if len(content) <= max_length:
            return content
        
        truncated = content[:max_length]
        # æ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„è¡Œæˆ–å¥å­
        last_newline = truncated.rfind('\n')
        if last_newline > max_length // 2:  # å¦‚æœæ‰¾åˆ°äº†åˆç†ä½ç½®çš„æ¢è¡Œç¬¦
            truncated = truncated[:last_newline]
        
        return f"{truncated}... [å†…å®¹è¢«æˆªæ–­ï¼Œæ€»é•¿åº¦: {len(content)} å­—ç¬¦]"
    
    def _get_hotspot_prompt_template(self) -> str:
        """è·å–çƒ­ç‚¹åˆ†æçš„â€œè¶…çº§æç¤ºè¯â€æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½ä¸ºé¡¶çº§æŠ€æœ¯å…¬å¸æœåŠ¡çš„èµ„æ·±è¡Œä¸šåˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä»¥ä¸‹æ¥è‡ªV2EXç¤¾åŒºçš„ã€å·²ç¼–å·çš„åŸå§‹è®¨è®ºææ–™ï¼Œå¹¶ä¸ºæŠ€æœ¯å†³ç­–è€…æ’°å†™ä¸€ä»½å¾ªåºæ¸è¿›ã€å¯è¿½æº¯æ¥æºçš„æƒ…æŠ¥ç®€æŠ¥ã€‚

**åŸå§‹è®¨è®ºææ–™:**
{content}

---

**ä½ çš„åˆ†æä»»åŠ¡:**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œåˆ†æå’Œå†…å®¹ç”Ÿæˆã€‚**è‡³å…³é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼šä½ çš„æ¯ä¸€æ¡åˆ†æã€æ´å¯Ÿå’Œå»ºè®®éƒ½å¿…é¡»åœ¨ç»“å°¾å¤„ä½¿ç”¨ `[Source: T_n]` æˆ– `[Sources: T_n, T_m]` çš„æ ¼å¼æ˜ç¡®æ ‡æ³¨å…¶ä¿¡æ¯æ¥æºã€‚**

**ç¬¬ä¸€é˜¶æ®µï¼šçƒ­é—¨ä¸»é¢˜é€Ÿè§ˆ (Top Topics Summary)**
é¦–å…ˆï¼Œè¯·é€šè¯»æ‰€æœ‰ææ–™ï¼Œå¯¹æ¯ä¸ªçƒ­é—¨ä¸»é¢˜è¿›è¡Œç®€æ˜æ‰¼è¦çš„æ€»ç»“ã€‚

**ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦æƒ…æŠ¥æ´å¯Ÿ (In-depth Intelligence Report)**
åœ¨å®Œæˆé€Ÿè§ˆåï¼Œè¯·è½¬æ¢è§†è§’ï¼ŒåŸºäºç¬¬ä¸€é˜¶æ®µä½ æ€»ç»“çš„æ‰€æœ‰ä¿¡æ¯ï¼Œè¿›è¡Œæ›´é«˜å±‚çº§çš„è¶‹åŠ¿åˆ†æå’Œæ´å¯Ÿæç‚¼ã€‚

---

**è¯·ä¸¥æ ¼éµç…§ä»¥ä¸‹Markdownæ ¼å¼è¾“å‡ºå®Œæ•´æŠ¥å‘Š:**

# ğŸ“ˆ V2EXç¤¾åŒºçƒ­ç‚¹ä¸æƒ…æŠ¥æ´å¯ŸæŠ¥å‘Š

## ğŸ”¥ æœ¬æ—¶æ®µçƒ­é—¨ä¸»é¢˜é€Ÿè§ˆ

[åœ¨æ­¤å¤„ç½—åˆ—æœ€é‡è¦çš„5-10ä¸ªçƒ­é—¨ä¸»é¢˜çš„é€Ÿè§ˆ]

### **1. [ä¸»é¢˜Açš„æ ‡é¢˜]**
*   **æ ¸å¿ƒå†…å®¹**: [å¯¹è¯¥ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹ã€è®¨è®ºç„¦ç‚¹å’Œä¸»è¦ç»“è®ºè¿›è¡Œ3-5å¥è¯çš„æ‘˜è¦ã€‚] [Source: T_n]

### **2. [ä¸»é¢˜Bçš„æ ‡é¢˜]**
*   **æ ¸å¿ƒå†…å®¹**: [å¯¹è¯¥ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹ã€è®¨è®ºç„¦ç‚¹å’Œä¸»è¦ç»“è®ºè¿›è¡Œ3-5å¥è¯çš„æ‘˜è¦ã€‚] [Source: T_m]

...(ä»¥æ­¤ç±»æ¨)

---

## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ (Executive Summary)

*   **[æ´å¯Ÿä¸€]**: [ç”¨ä¸€å¥è¯é«˜åº¦æ¦‚æ‹¬ä½ å‘ç°çš„æœ€é‡è¦çš„è¶‹åŠ¿æˆ–æ´å¯Ÿã€‚ä¾‹å¦‚ï¼šå¯¹äº‘æœåŠ¡æˆæœ¬å’Œæ›¿ä»£æ–¹æ¡ˆçš„è®¨è®ºæ¿€å¢ï¼Œåæ˜ å‡ºå¼€å‘è€…å¯¹æˆæœ¬æ§åˆ¶çš„æ™®éç„¦è™‘ã€‚] [Sources: T1, T5, T8]
*   **[æ´å¯ŸäºŒ]**: [ç¬¬äºŒä¸ªé‡è¦æ´å¯Ÿã€‚ä¾‹å¦‚ï¼šAI Agentçš„å®ç°å’Œåº”ç”¨æˆä¸ºæ–°çš„æŠ€æœ¯ç„¦ç‚¹ï¼Œå¤šä¸ªçƒ­é—¨é¡¹ç›®å›´ç»•æ­¤å±•å¼€ã€‚] [Sources: T2, T9]
*   **[æ´å¯Ÿä¸‰]**: [ç¬¬ä¸‰ä¸ªé‡è¦æ´å¯Ÿã€‚] [Source: T4]

## ğŸ” è¶‹åŠ¿ä¸ä¿¡å·åˆ†æ (Trends & Signals Analysis)

### ğŸš€ æ–°å…´æŠ€æœ¯ä¸å·¥å…·é£å‘
*   **[æŠ€æœ¯/å·¥å…·A]**: [æè¿°å®ƒæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆå®ƒç°åœ¨å¾ˆçƒ­é—¨ï¼Œä»¥åŠåœ¨è®¨è®ºä¸­æ˜¯å¦‚ä½•ä½“ç°çš„ã€‚] [Source: T3]
*   **[æŠ€æœ¯/å·¥å…·B]**: [åŒä¸Šã€‚] [Source: T7]

### ğŸ”— è®¨è®ºçƒ­ç‚¹çš„å†…åœ¨å…³è”
*   **[å…³è”æ€§åˆ†æ]**: [è¯¦ç»†é˜è¿°ä½ å‘ç°çš„ä¸åŒçƒ­ç‚¹ä¹‹é—´çš„è”ç³»ã€‚ä¾‹å¦‚ï¼šå¯¹â€œXXäº‘æœåŠ¡é«˜æ˜‚è´¹ç”¨â€çš„æŠ±æ€¨ï¼ˆä¸»é¢˜Aï¼‰ä¸â€œYYå¼€æºæ›¿ä»£æ–¹æ¡ˆâ€çš„å‡ºç°ï¼ˆä¸»é¢˜Bï¼‰å½¢æˆäº†å‘¼åº”ï¼Œå…±åŒæŒ‡å‘äº†å¼€å‘è€…å¯¹åŸºç¡€è®¾æ–½æˆæœ¬ä¼˜åŒ–çš„æ¢ç´¢ã€‚] [Sources: T1, T6]

### âš ï¸ æ™®éç—›ç‚¹ä¸æ½œåœ¨éœ€æ±‚
*   **[ç—›ç‚¹ä¸€]**: [æè¿°ç¤¾åŒºå¼€å‘è€…æ™®éé‡åˆ°çš„ä¸€ä¸ªé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚] [Source: T5]
*   **[ç—›ç‚¹äºŒ]**: [åŒä¸Šã€‚] [Source: T10]

##  actionable å»ºè®® (Actionable Recommendations)

*   **å¯¹äºå¼€å‘è€…**: [åŸºäºä»¥ä¸Šåˆ†æï¼Œç»™ä¸ªäººå¼€å‘è€…æå‡º1-2æ¡å…·ä½“å»ºè®®ã€‚ä¾‹å¦‚ï¼šå»ºè®®å…³æ³¨XXæŠ€æœ¯ï¼Œå°è¯•å°†YYå·¥å…·é›†æˆåˆ°å½“å‰å·¥ä½œæµä¸­ä»¥æé«˜æ•ˆç‡ã€‚] [Sources: T3, T7]
*   **å¯¹äºæŠ€æœ¯å›¢é˜Ÿ**: [ç»™æŠ€æœ¯å›¢é˜Ÿæˆ–å†³ç­–è€…æå‡º1-2æ¡å»ºè®®ã€‚ä¾‹å¦‚ï¼šå»ºè®®è¯„ä¼°å¼•å…¥XXè§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œä»¥è§£å†³å›¢é˜Ÿåœ¨YYæ–¹é¢é‡åˆ°çš„æ™®éé—®é¢˜ã€‚] [Source: T5]
"""    

    def _format_topics_for_analysis(self, hot_topics_data: List[Dict[str, Any]]) -> str:
        """å°†æ‰€æœ‰çƒ­é—¨ä¸»é¢˜åˆå¹¶ä¸ºä¸€ä¸ªæ–‡æ¡£ç”¨äºLLMç»Ÿä¸€åˆ†æ"""
        content_parts = [
            f"""=== V2EXçƒ­é—¨ä¸»é¢˜ç»¼åˆåˆ†ææ–‡æ¡£ ===""",
            f"æ€»è®¡ {len(hot_topics_data)} ä¸ªçƒ­é—¨ä¸»é¢˜",
            "",
        ]
        
        for i, topic_data in enumerate(hot_topics_data, 1):
            topic = topic_data['topic']
            replies = topic_data['replies']
            
            content_parts.extend([
                f"\n### [Source: T{i}] {topic['title']}",
                f"- èŠ‚ç‚¹: {topic['node_name']}",
                f"- ä½œè€…: {topic['member_username']}",
                f"- å›å¤æ•°: {topic['replies']}",
                f"- æ„Ÿè°¢æ•°: {topic['total_thanks_count']}",
                f"- çƒ­åº¦: {topic['hotness_score']:.2f}",
                f"- URL: {topic['url']}",
                ""
            ])
            
            if topic.get('content'):
                content = topic['content'].strip()
                if len(content) > 800: content = content[:800] + "..."
                content_parts.extend(["**ä¸»è´´å†…å®¹:**", content, ""])
            
            if replies:
                content_parts.append("**çƒ­é—¨å›å¤:**")
                for j, reply in enumerate(replies[:5], 1): # Limit to 5 replies
                    if reply.get('content'):
                        reply_content = reply['content'].strip()
                        if len(reply_content) > 200: reply_content = reply_content[:200] + "..."
                        thanks_info = f"(æ„Ÿè°¢: {reply['thanks_count']})" if reply['thanks_count'] > 0 else ""
                        content_parts.append(f"{j}. {reply['member_username']} {thanks_info}: {reply_content}")
                content_parts.append("")

            content_parts.append("---\n")
        
        full_content = "\n".join(content_parts)
        self.logger.info(f"æ ¼å¼åŒ–åçš„ä¸»é¢˜å†…å®¹æ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
        return self._truncate_unified_content(full_content) # Still need truncation
    
    def _truncate_unified_content(self, content: str) -> str:
        """æˆªæ–­ç»Ÿä¸€åˆ†æå†…å®¹åˆ°åˆé€‚é•¿åº¦"""
        if len(content) <= self.max_content_length:
            return content
        
        # æ™ºèƒ½æˆªæ–­ï¼šå°è¯•åœ¨ä¸»é¢˜åˆ†éš”ç¬¦å¤„æˆªæ–­
        truncated = content[:self.max_content_length]
        
        # å°è¯•åœ¨ä¸»é¢˜åˆ†éš”ç¬¦"---"å¤„æˆªæ–­
        last_separator = truncated.rfind('---')
        if last_separator > self.max_content_length * 0.7:
            truncated = truncated[:last_separator]
        else:
            # å°è¯•åœ¨æ®µè½åˆ†éš”ç¬¦å¤„æˆªæ–­
            for delimiter in ['\n\n', '\n', 'ã€‚', '.']:
                last_pos = truncated.rfind(delimiter)
                if last_pos > self.max_content_length * 0.8:
                    truncated = truncated[:last_pos + len(delimiter)]
                    break
        
        self.logger.info(f"ç»Ÿä¸€åˆ†æå†…å®¹è¢«æˆªæ–­: {len(content)} -> {len(truncated)} å­—ç¬¦")
        return truncated + "\n\n...[å†…å®¹è¿‡é•¿å·²è¢«æˆªæ–­]"
    
    def generate_node_report(self, node_name: str, hours_back: int = 24, 
                           report_type: str = 'hotspot') -> Dict[str, Any]:
        """
        ä¸ºæŒ‡å®šèŠ‚ç‚¹ç”Ÿæˆåˆ†ææŠ¥å‘Š
        """
        try:
            self.logger.info(f"å¼€å§‹ä¸ºèŠ‚ç‚¹ '{node_name}' ç”Ÿæˆ {report_type} æŠ¥å‘Š")
            
            # æ—¶é—´èŒƒå›´
            end_time = self.get_beijing_time()
            start_time = end_time - timedelta(hours=hours_back)
            
            # è·å–çƒ­é—¨ä¸»é¢˜
            hot_topics = self.db.get_hot_topics_by_node(
                node_name=node_name, 
                limit=self.top_topics_per_node,
                period_hours=hours_back
            )
            
            if not hot_topics:
                self.logger.warning(f"èŠ‚ç‚¹ '{node_name}' åœ¨è¿‡å» {hours_back} å°æ—¶å†…æ— çƒ­é—¨ä¸»é¢˜")
                return {
                    'success': False,
                    'error': f"èŠ‚ç‚¹ '{node_name}' æ— çƒ­é—¨å†…å®¹",
                    'node_name': node_name
                }
            
            # æ‰¹é‡è·å–ä¸»é¢˜è¯¦ç»†ä¿¡æ¯å’Œå›å¤ï¼Œé¿å…N+1æŸ¥è¯¢
            self.logger.info("å¼€å§‹æ‰¹é‡è·å–ä¸»é¢˜è¯¦æƒ…...")
            import time
            start_time_db = time.time()
            topic_ids = [topic['id'] for topic in hot_topics]
            hot_topics_data = self.db.get_topics_with_replies_batch(
                topic_ids,
                reply_limit=self.top_replies_per_topic
            )
            db_duration = time.time() - start_time_db
            self.logger.info(f"æ‰¹é‡è·å–ä¸»é¢˜è¯¦æƒ…å®Œæˆï¼Œè€—æ—¶ {db_duration:.2f} ç§’")

            if not hot_topics_data:
                return {
                    'success': False,
                    'error': f"æ— æ³•è·å–èŠ‚ç‚¹ '{node_name}' çš„ä¸»é¢˜è¯¦æƒ…",
                    'node_name': node_name
                }
            
            # ç›´æ¥è°ƒç”¨ç»Ÿä¸€æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
            return self._generate_unified_report(
                node_name=node_name,
                hot_topics_data=hot_topics_data,
                start_time=start_time,
                end_time=end_time,
                report_type=report_type
            )
            
        except Exception as e:
            error_msg = f"ç”ŸæˆèŠ‚ç‚¹ '{node_name}' æŠ¥å‘Šå¤±è´¥: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return {
                'success': False,
                'error': error_msg,
                'node_name': node_name
            }
    
    def generate_global_report(self, hours_back: int = 24, 
                             report_type: str = 'hotspot') -> Dict[str, Any]:
        """
        ç”Ÿæˆå…¨ç«™çƒ­ç‚¹æŠ¥å‘Š
        
        Args:
            hours_back: å›æº¯æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            report_type: æŠ¥å‘Šç±»å‹
            
        Returns:
            æŠ¥å‘Šç”Ÿæˆç»“æœ
        """
        try:
            self.logger.info(f"å¼€å§‹ç”Ÿæˆå…¨ç«™ {report_type} æŠ¥å‘Š")
            
            # æ—¶é—´èŒƒå›´
            end_time = self.get_beijing_time()
            start_time = end_time - timedelta(hours=hours_back)
            
            # è·å–å…¨ç«™çƒ­é—¨ä¸»é¢˜
            hot_topics = self.db.get_hot_topics_by_node(
                node_name=None,  # Noneè¡¨ç¤ºå…¨ç«™
                limit=self.top_topics_per_node,
                period_hours=hours_back
            )
            
            if not hot_topics:
                return {
                    'success': False,
                    'error': f"å…¨ç«™åœ¨è¿‡å» {hours_back} å°æ—¶å†…æ— çƒ­é—¨ä¸»é¢˜"
                }
            
            # æ‰¹é‡è·å–ä¸»é¢˜è¯¦ç»†ä¿¡æ¯
            self.logger.info("å¼€å§‹æ‰¹é‡è·å–ä¸»é¢˜è¯¦æƒ…...")
            import time
            start_time_db = time.time()
            topic_ids = [topic['id'] for topic in hot_topics]
            hot_topics_data = self.db.get_topics_with_replies_batch(
                topic_ids,
                reply_limit=self.top_replies_per_topic
            )
            db_duration = time.time() - start_time_db
            self.logger.info(f"æ‰¹é‡è·å–ä¸»é¢˜è¯¦æƒ…å®Œæˆï¼Œè€—æ—¶ {db_duration:.2f} ç§’")
            
            # ä½¿ç”¨ä¸èŠ‚ç‚¹æŠ¥å‘Šç›¸åŒçš„é€»è¾‘
            return self._generate_unified_report(
                node_name="å…¨ç«™",
                hot_topics_data=hot_topics_data,
                start_time=start_time,
                end_time=end_time,
                report_type=report_type
            )
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå…¨ç«™æŠ¥å‘Šå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg
            }
    
    def _generate_unified_report(self, node_name: str, hot_topics_data: List[Dict[str, Any]],
                               start_time: datetime, end_time: datetime, 
                               report_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿä¸€æŠ¥å‘Šçš„å†…éƒ¨æ–¹æ³•ï¼ˆå·²å‡çº§ï¼‰"""
        
        # ä½¿ç”¨æ–°çš„æ–¹æ³•æ ¼å¼åŒ–æ‰€æœ‰ä¸»é¢˜å†…å®¹
        unified_content = self._format_topics_for_analysis(hot_topics_data)

        # LLMåˆ†æ
        if not self.llm:
            return {'success': False, 'error': 'LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–'}

        # è·å–æŠ¥å‘Šç±»å‹çš„å¯¹åº”prompt
        if report_type == 'hotspot':
            prompt_template = self._get_hotspot_prompt_template()
        else:
            # ä¸ºå…¶ä»–æŠ¥å‘Šç±»å‹å¯ä»¥å®šä¹‰ä¸åŒçš„promptï¼Œæ­¤å¤„ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é»˜è®¤å€¼
            self.logger.warning(f"æŠ¥å‘Šç±»å‹ '{report_type}' æ²¡æœ‰ä¸“é—¨çš„Promptæ¨¡æ¿ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†æã€‚")
            prompt_template = "è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹çš„ä¸»è¦çœ‹ç‚¹ã€äº‰è®®å’Œç»“è®ºï¼š\n\n{content}"

        llm_result = self.llm.analyze_content(unified_content, prompt_template)
        
        if not llm_result.get('success'):
            return {
                'success': False,
                'error': f"LLMåˆ†æå¤±è´¥: {llm_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            }
        
        # ç”ŸæˆæŠ¥å‘Šæ ‡é¢˜
        if node_name == "å…¨ç«™":
            report_title = f"ğŸŒŸ V2EXå…¨ç«™çƒ­ç‚¹æ´å¯ŸæŠ¥å‘Š"
        else:
            report_title = f"ğŸ“ˆ [{node_name}]èŠ‚ç‚¹çƒ­ç‚¹æ´å¯ŸæŠ¥å‘Š"
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        markdown_report = self._generate_markdown_report(
            node_name=node_name,
            analysis_result=llm_result,
            hot_topics_data=hot_topics_data,
            start_time=start_time,
            end_time=end_time,
            report_title=report_title,
            report_type=report_type
        )
        
        # ä¿å­˜æŠ¥å‘Š
        report_data = {
            'node_name': node_name,
            'report_type': report_type,
            'analysis_period_start': start_time,
            'analysis_period_end': end_time,
            'topics_analyzed': len(hot_topics_data),
            'report_title': report_title,
            'report_content': markdown_report,
            'generated_at': self.get_beijing_time()
        }
        
        report_id = self.db.insert_report(report_data)
        
        final_result = {
            'success': True,
            'report_id': report_id,
            'node_name': node_name,
            'report_type': report_type,
            'topics_analyzed': len(hot_topics_data),
            'report_title': report_title,
            'report_content': markdown_report,
            'report_content_preview': self._truncate_content_for_logging(markdown_report, 300),
            'analysis_provider': llm_result.get('provider'),
            'analysis_model': llm_result.get('model'),
            'generated_at': report_data['generated_at']
        }

        if llm_result.get('partial'):
            final_result['success'] = False
            final_result['error'] = "éƒ¨åˆ†ç»“æœï¼šLLMè¿æ¥ä¸­æ–­"
        
        return final_result
    
    def _generate_markdown_report(self, node_name: str, analysis_result: Dict[str, Any],
                                hot_topics_data: List[Dict[str, Any]], start_time: datetime,
                                end_time: datetime, report_title: str, 
                                report_type: str) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        
        # æ—¶é—´ä¿¡æ¯
        start_str = start_time.strftime('%Y-%m-%d %H:%M:%S')
        end_str = end_time.strftime('%Y-%m-%d %H:%M:%S')
        generate_time = self.get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')
        
        # æ„å»ºæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"# {report_title}",
            "",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {generate_time}*",
            f"*æ•°æ®èŒƒå›´: {start_str} - {end_str}*",
            "",
            "---",
            "",
            analysis_result.get('analysis', 'åˆ†æå†…å®¹ç”Ÿæˆå¤±è´¥ã€‚'),
            "",
            "---",
            "",
            "## ğŸ“š æ¥æºæ¸…å• (Source List)",
            ""
        ]
        
        # ç”Ÿæˆæ¥æºæ¸…å•
        for i, topic_data in enumerate(hot_topics_data, 1):
            topic_info = topic_data['topic']
            report_lines.append(
                f"- **[T{i}]**: [{topic_info['title']}]({topic_info['url']}) "
                f"({topic_info['node_name']} | {topic_info['replies']}å›å¤ | {topic_info['total_thanks_count']}æ„Ÿè°¢)"
            )
        
        report_lines.extend(["", "---", ""])
        
        # æŠ€æœ¯ä¿¡æ¯
        if analysis_result.get('provider'):
            report_lines.append(
                f"*åˆ†æå¼•æ“: {analysis_result['provider']} ({analysis_result.get('model', 'unknown')})*"
            )
        
        report_lines.extend([
            "",
            f"ğŸ“Š **ç»Ÿè®¡æ‘˜è¦**: æœ¬æŠ¥å‘Šåˆ†æäº† {len(hot_topics_data)} ä¸ªçƒ­é—¨ä¸»é¢˜",
            ""
        ])

        if analysis_result.get('partial'):
            report_lines.append("")
            report_lines.append("*æ³¨æ„ï¼šç”±äºä¸åˆ†æå¼•æ“çš„è¿æ¥æ„å¤–ä¸­æ–­ï¼Œæ­¤æŠ¥å‘Šå¯èƒ½ä¸å®Œæ•´ã€‚*")

        report_lines.extend([
            "",
            "*æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*"
        ])
        
        return "\n".join(report_lines)


# å…¨å±€æŠ¥å‘Šç”Ÿæˆå™¨å®ä¾‹
report_generator = V2EXReportGenerator()