name: V2EX Info Collector

on:
  schedule:
    # 每30分钟执行一次增量更新
    - cron: '*/30 * * * *'
    # 每天北京时间 6:00 执行完整更新（包括节点更新）
    - cron: '0 22 * * *'
  workflow_dispatch:
    inputs:
      task:
        description: '任务类型'
        required: true
        default: 'crawl'
        type: choice
        options:
        - crawl
        - cleanup
        - stats
        - full
      retention_days:
        description: '数据保留天数（仅cleanup任务）'
        required: false
        type: number
        default: 90

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run crawler
      env:
        # 数据库配置
        DB_HOST: ${{ secrets.DB_HOST }}
        DB_PORT: ${{ secrets.DB_PORT }}
        DB_USER: ${{ secrets.DB_USER }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        DB_NAME: ${{ secrets.DB_NAME }}
        DB_SSL_MODE: ${{ secrets.DB_SSL_MODE }}
        
        # 爬虫配置
        CRAWLER_DELAY_SECONDS: ${{ secrets.CRAWLER_DELAY_SECONDS }}
        CRAWLER_MAX_RETRIES: ${{ secrets.CRAWLER_MAX_RETRIES }}
        CRAWLER_TIMEOUT_SECONDS: ${{ secrets.CRAWLER_TIMEOUT_SECONDS }}
        CRAWLER_MAX_PAGES_PER_NODE: ${{ secrets.CRAWLER_MAX_PAGES_PER_NODE }}
        CRAWLER_FETCH_REPLIES: ${{ secrets.CRAWLER_FETCH_REPLIES }}
        CRAWLER_MAX_CONCURRENT_NODES: ${{ secrets.CRAWLER_MAX_CONCURRENT_NODES }}
        CRAWLER_MAX_CONCURRENT_REPLIES: ${{ secrets.CRAWLER_MAX_CONCURRENT_REPLIES }}
        
        # 数据保留配置
        DATA_RETENTION_DAYS: ${{ secrets.DATA_RETENTION_DAYS }}
        
        # 日志配置
        LOGGING_LOG_LEVEL: ${{ secrets.LOGGING_LOG_LEVEL }}
        
        # 目标节点配置
        TARGETS: ${{ secrets.TARGETS }}
        
      run: |
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          if [ -n "${{ github.event.inputs.retention_days }}" ]; then
            python main.py --task ${{ github.event.inputs.task }} --retention-days ${{ github.event.inputs.retention_days }}
          else
            python main.py --task ${{ github.event.inputs.task }}
          fi
        elif [ "${{ github.event.schedule }}" = "0 22 * * *" ]; then
          # 每天6点执行完整更新（包括节点）
          python main.py --task full --force-nodes
        else
          # 每30分钟执行增量更新
          python main.py --task crawl
        fi
